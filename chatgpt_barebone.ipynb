{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up env variables (e.g. in a .env file if using vscode)\n",
    "\n",
    "- \"KEY\", for your openai API key\n",
    "- \"PROMPT_SYSTEM\" (optionally) if you want to give your chatbot a default personality (e.g. helpful expert in area x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dependencies\n",
    "\n",
    "#%pip install openai\n",
    "#%pip install ipywidgets\n",
    "#%pip install pyperclip"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import openai\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import pyperclip\n",
    "\n",
    "#api key\n",
    "openai.api_key = os.getenv(\"KEY\")\n",
    "#default personality / behavior\n",
    "default_system = os.getenv(\"PROMPT_SYSTEM\")\n",
    "\n",
    "#chat model parameters (ChatGPT)\n",
    "chat_model = \"gpt-3.5-turbo\"\n",
    "chat_temp = 0.7\n",
    "chat_convo_limit = 3000\n",
    "chat_token_limit = 1000\n",
    "\n",
    "if (default_system is not None):\n",
    "    #initializing chat history with the system prompt\n",
    "    conversation = [{\"role\": \"system\", \"content\": default_system}]\n",
    "else:\n",
    "    conversation = []\n",
    "    \n",
    "#the maximum context window (short term memory) is 4096 tokens, so we need to keep track of token usage to limit the risk running into an API error; the webUI is using a sliding window to manage around this limit\n",
    "token_count = 0\n",
    "\n",
    "\n",
    "#API calls to OpenAI\n",
    "def get_gpt (prompt):\n",
    "    global token_count\n",
    "    global conversation\n",
    "\n",
    "    #if our current token_count is above limit then we remove the oldest message from the conversation, if there is a personality set up in default_system then the 2nd oldest\n",
    "    if token_count > chat_convo_limit:\n",
    "   \n",
    "        prompt_length = len(prompt)\n",
    "\n",
    "        while prompt_length > 0 :\n",
    "   \n",
    "            if (default_system is not None):\n",
    "                item_length = len(conversation[1]['content'])\n",
    "                conversation.pop(1)\n",
    "            else:\n",
    "                item_length = len(conversation[0]['content'])\n",
    "                conversation.pop(0)\n",
    "\n",
    "            prompt_length = prompt_length - item_length\n",
    "        \n",
    "\n",
    "    conversation.append({\"role\": \"user\", \"content\": prompt})\n",
    "    response = openai.ChatCompletion.create(\n",
    "            #gpt-3.5-turbo is the current equivalent of what's behind the web UI\n",
    "            model=chat_model, \n",
    "            messages=conversation,\n",
    "            #the higher the temperature the more creative / diverse the responses will be, minimum is 0 (deterministic), maximum is currently 2 (too much)\n",
    "            temperature = chat_temp,\n",
    "            max_tokens = chat_token_limit,\n",
    "            n = 1            \n",
    "        )\n",
    "    message = response.choices[0].message.content\n",
    "    token_count = response.usage.total_tokens\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": message})\n",
    "    return response\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#HTML formatting for the output window\n",
    "html_template = \"\"\"\n",
    "<style>\n",
    "div {{white-space: pre-wrap;font-size: 18px;}}\n",
    "</style>\n",
    "<div>\n",
    "<div>\n",
    "{}\n",
    "</div>\n",
    "<div>\n",
    "{}\n",
    "</div>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "#displaying messages\n",
    "def render_message(message):\n",
    "    if message[\"role\"] == \"assistant\":\n",
    "        return(f\"<i>ChatGPT</i>: {message['content']}\")\n",
    "    else:\n",
    "        return(f\"<b>You</b>: {message['content']}\")\n",
    "    \n",
    "\n",
    "# handler for copy button's on_click event\n",
    "def handle_copy(button):\n",
    "    global conversation\n",
    "    \n",
    "    if (default_system is not None):\n",
    "        conversation_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation[1:]])\n",
    "    else:\n",
    "        conversation_str = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in conversation])\n",
    "\n",
    "    # Add conversation string to clipboard\n",
    "    pyperclip.copy(conversation_str)\n",
    "\n",
    "\n",
    "# handler for send button's on_click event\n",
    "def handle_input(button):\n",
    "    user_input = input_widget.value\n",
    "    output.append_display_data(\"--------------message sent--------------\")\n",
    "    input_widget.value = ''\n",
    "    \n",
    "    response = get_gpt(user_input)\n",
    "    latest_messages = html_template.format(render_message(conversation[-2]),render_message(conversation[-1]))\n",
    "    output.append_display_data(HTML(latest_messages))\n",
    "\n",
    "    print('current token count: ', token_count)\n",
    "\n",
    "\n",
    "output = widgets.Output(layout={'border': '1px solid black','width': '100%'},contextmenu=True)\n",
    "\n",
    "#input window\n",
    "input_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Type your message here...',\n",
    "    description='Input:',\n",
    "    disabled=False,\n",
    "    layout={'width': '100%', 'height': 'auto'},\n",
    "    rows = 1\n",
    ")\n",
    "\n",
    "#resizing input window as the input gets longer    \n",
    "def get_bigger(args):        \n",
    "    input_widget.rows = input_widget.value.count('\\n') + 1\n",
    "\n",
    "input_widget.observe(get_bigger,'value')\n",
    "\n",
    "#the send button\n",
    "send_button = widgets.Button(description='Send', layout={'width': '100%'})\n",
    "send_button.on_click(handle_input)\n",
    "\n",
    "#the copy to clipbord button\n",
    "copy_button = widgets.Button(description='Copy Chat to Clipboard', layout={'width': '100%'})\n",
    "copy_button.on_click(handle_copy)\n",
    "\n",
    "# Display the UI\n",
    "display(widgets.VBox([copy_button,output,input_widget,send_button]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
